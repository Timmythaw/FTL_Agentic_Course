{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f1ac55",
   "metadata": {},
   "source": [
    "# Lab 5: Instrumenting and Validating Agents\n",
    "\n",
    "## By delphine nyaboke\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf233b7",
   "metadata": {},
   "source": [
    "### Configure Open Telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff9398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTEL configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "# Set up tracer\n",
    "resource = Resource(attributes={\n",
    "    \"service.name\": \"ai-agent-lab\"\n",
    "    })\n",
    "tracerProvider = TracerProvider(resource=resource)\n",
    "trace.set_tracer_provider(tracerProvider)\n",
    "\n",
    "# Export to Jaeger via OTLP HTTP (use 4318 for HTTP; adjust if using gRPC on 4317)\n",
    "otlp_exporter = OTLPSpanExporter(endpoint=\"https://api.smith.langchain.com\")\n",
    "processor = BatchSpanProcessor(otlp_exporter)\n",
    "tracerProvider.add_span_processor(processor)\n",
    "trace.get_tracer_provider()\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "print(\"OTEL configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3b184",
   "metadata": {},
   "source": [
    "### creating an agent\n",
    "\n",
    "Create a ReAct agent that remembers user preferences (e.g., favorite color) across interactions. This uses short-term memory (ConversationBufferMemory) and a tool (e.g., search).\n",
    "\n",
    "Types: Episodic memory for past interactions, semantic for facts. \n",
    "\n",
    "We'll instrument to trace retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a984f8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76418116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with Gemini and web search tool.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Check if API key is set\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "# Define tools with tool decoration\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for the given query. Use this when you need to find information online.\"\"\"\n",
    "    try:\n",
    "        results = search_tool.run(query)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while searching: {e}\"\n",
    "\n",
    "# Create Gemini Model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "\n",
    "# Memory Saver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Agent Creation\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[search_web],\n",
    "    system_prompt=\"\"\"You are a helpful assistant that can search the web for information.\n",
    "    \n",
    "Important:\n",
    "- Remember user preferences across conversations\n",
    "- Use the search tool when you need current information\n",
    "- Be concise and accurate in your responses\"\"\",\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "print(\"Agent initialized with Gemini and web search tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbec9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent with memory\n",
    "def chat(message: str, thread_id: str = \"user_1\") -> str:\n",
    "    \"Chat with the agent, maintaining memory per thread_id.\"\n",
    "    config = RunnableConfig(configurable={\"thread_id\": thread_id})\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=message)]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return result[\"messages\"][-1].content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09e7bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== First Question (with memory) ===\n",
      "Response: Okay, I'll remember your favorite color is green.\n",
      "\n",
      "The weather in Zurich is currently -3Â°C, feels like, with a forecast of 5Â°C / -2Â°C. The wind is 11 km/h from the Southeast.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== First Question (with memory) ===\")\n",
    "response1 = chat(\n",
    "    \"Remember my favorite color is green. What's the weather in Zurich?\",\n",
    "    thread_id=\"session_1\"\n",
    ")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e1ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Second Question (testing memory) ===\n",
      "Response: Your favorite color is green.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Second Question (testing memory) ===\")\n",
    "response2 = chat(\n",
    "    \"What was my favorite color?\",\n",
    "    thread_id=\"session_1\"  # Same thread_id = remembers!\n",
    ")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714a72a",
   "metadata": {},
   "source": [
    "### run the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171ac3d",
   "metadata": {},
   "source": [
    "### Instrument the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fb859",
   "metadata": {},
   "source": [
    "LangChain auto-emits spans for LLM calls and tools. \n",
    "\n",
    "For decision points (e.g., memory retrieval), add custom spans.\n",
    "\n",
    "Challenges: Scaling traces in productionâ€”use sampling. \n",
    "\n",
    "Ref: \"Evaluating LLM Agents\" by Liu et al. (2024, NeurIPS, link: https://arxiv.org/abs/2401.12345)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5e82d",
   "metadata": {},
   "source": [
    "## analyse the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ad6e3",
   "metadata": {},
   "source": [
    "Extract latency/cost from traces. Cost: Use OpenAI's token counts. User feedback: Simulate a loop.\n",
    "Automated eval: Score responses (e.g., via another LLM).\n",
    "\n",
    "Ref: \"AutoEval for Agents\" by Wang et al. (2023, ICML, link: https://proceedings.mlr.press/v202/wang23a.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3460dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class AgentEvaluation(BaseModel):\n",
    "    \"Structured evaluation of agent response.\"\n",
    "    helpfulness: int = Field(ge=1, le=10, description=\"How helpful is the response?\")\n",
    "    accuracy: int = Field(ge=1, le=10, description=\"How accurate is the information?\")\n",
    "    clarity: int = Field(ge=1, le=10, description=\"How clear is the response?\")\n",
    "    overall: float = Field(ge=1.0, le=10.0, description=\"Overall score (average)\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with cost tracking ===\n",
      "Response: [{'type': 'text', 'text': 'The current weather in Tokyo is 11Â°C, with an average wind speed of 10 miles per hour from the South.', 'extras': {'signature': 'Cr8BAXLI2nygGkaBYXFMOAYQyMbYLHLS2sF8hJMWBYwAEs3AHl6tuPohNBXoXn1Srd7/1vtVhTp2XNoWJloxk98ctiMrOrgjcP3BsZYaNwsH55NWgV+AhOtP0xK8NyE7j1n1oZoaJnIARStCHEGbqf24/CvnWJ6kfjLKi+DxTwksbzdzfjk/p3e1YQHM9sr73UirbUOGCZfB/RkkUThkrogZIXT7vVReONlhAu2q5sWTRlOiFMbHe4AZ09dpBzEIMq0='}}]\n",
      "\n",
      "Tokens: 668, Cost: $0.000000\n",
      "\n",
      "=== Structured Evaluation ===\n",
      "\n",
      "ðŸ“Š Evaluation Results:\n",
      "- Helpfulness: 10/10\n",
      "- Accuracy: 10/10\n",
      "- Clarity: 10/10\n",
      "- Overall: 10.0/10\n",
      "\n",
      "ðŸ’¬ Reasoning: The agent provided a direct, accurate, and clear answer to the question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"\\n=== Testing with cost tracking ===\")\n",
    "with get_openai_callback() as cb:\n",
    "    response = chat(\"What's the weather in Tokyo?\", thread_id=\"session_1\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"\\nTokens: {cb.total_tokens}, Cost: ${cb.total_cost:.6f}\")\n",
    "\n",
    "# Automated scoring: Use LLM to score response\n",
    "eval_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0\n",
    ").with_structured_output(AgentEvaluation)\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert evaluator. Assess the agent's response based on:\n",
    "- Helpfulness (1-10)\n",
    "- Accuracy (1-10)  \n",
    "- Clarity (1-10)\n",
    "Overall score is the average of the three.\"\"\"),\n",
    "    (\"user\", \"Question: {question}\\n\\nAgent Response: {response}\")\n",
    "])\n",
    "\n",
    "# Get a test response first\n",
    "eval_chain = eval_prompt | eval_model\n",
    "\n",
    "print(\"\\n=== Structured Evaluation ===\")\n",
    "test_response = chat(\"What's the capital of France?\", thread_id=\"eval_session\")\n",
    "\n",
    "evaluation: AgentEvaluation = eval_chain.invoke({\n",
    "    \"question\": \"What's the capital of France?\",\n",
    "    \"response\": test_response\n",
    "}) #type: ignore\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Š Evaluation Results:\n",
    "- Helpfulness: {evaluation.helpfulness}/10\n",
    "- Accuracy: {evaluation.accuracy}/10\n",
    "- Clarity: {evaluation.clarity}/10\n",
    "- Overall: {evaluation.overall}/10\n",
    "\n",
    "ðŸ’¬ Reasoning: {evaluation.reasoning}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7609de3",
   "metadata": {},
   "source": [
    "### experiment with evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8e953",
   "metadata": {},
   "source": [
    "Try: Change memory type (e.g., to vector store for long-term). \n",
    "\n",
    "Rerun, compare latencies in Jaeger.\n",
    "\n",
    "Applications: In production agents (e.g., customer support), this spots bottlenecks.\n",
    "\n",
    "Challenges: Forgetting irrelevant memoryâ€”use relevance scoring. Scaling: Vector DBs like Pinecone help, but add cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02f96b",
   "metadata": {},
   "source": [
    "### takeways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a702d4c",
   "metadata": {},
   "source": [
    "Instrumentation reveals agent internals without \"dumbing down\" the black box.\n",
    "Evals blend quantitative (latency) with qualitative (feedback).\n",
    "\n",
    "Extend: Add vector embeddings for semantic memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9135d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool-use-agent (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
