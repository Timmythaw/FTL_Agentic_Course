{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f1ac55",
   "metadata": {},
   "source": [
    "# Lab 5: Instrumenting and Validating Agents\n",
    "\n",
    "## By delphine nyaboke\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf233b7",
   "metadata": {},
   "source": [
    "### Configure Open Telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff9398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTEL configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "# Set up tracer\n",
    "resource = Resource(attributes={\"service.name\": \"ai-agent-lab\"})\n",
    "provider = TracerProvider(resource=resource)\n",
    "\n",
    "# Export to Jaeger via OTLP HTTP (use 4318 for HTTP; adjust if using gRPC on 4317)\n",
    "otlp_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")\n",
    "provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n",
    "\n",
    "# Set the provider after adding the processor\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# Instrument LangChain with built-in OTEL support\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"http://localhost:4318\"  # Match OTLP HTTP for consistency\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"dummy\"  # Not needed for local\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "print(\"OTEL configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3b184",
   "metadata": {},
   "source": [
    "### creating an agent\n",
    "\n",
    "Create a ReAct agent that remembers user preferences (e.g., favorite color) across interactions. This uses short-term memory (ConversationBufferMemory) and a tool (e.g., search).\n",
    "\n",
    "Types: Episodic memory for past interactions, semantic for facts. \n",
    "\n",
    "We'll instrument to trace retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a984f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Disable LangSmith tracing since we don't have the endpoint running\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_classic.agents import AgentExecutor, create_react_agent  \n",
    "# from langchain_classic.memory import ConversationBufferMemory\n",
    "# from langchain_core.tools import Tool\n",
    "# from langchain_community.tools import DuckDuckGoSearchRun\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# # Check if API key is loaded\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not api_key:\n",
    "#     raise ValueError(\"OPENAI_API_KEY not found in .env file. Please add it.\")\n",
    "\n",
    "# # Simple tracer setup without any exporters\n",
    "# trace.set_tracer_provider(trace.NoOpTracerProvider())  # Disable all exports\n",
    "# tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# # Tools\n",
    "# search = DuckDuckGoSearchRun()\n",
    "# tools = [Tool.from_function(name=\"Search\", func=search.run, description=\"Search the web\")]\n",
    "\n",
    "# # Memory\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"input\")\n",
    "\n",
    "# # Prompt template\n",
    "# template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "# {tools}\n",
    "\n",
    "# Use the following format:\n",
    "\n",
    "# Question: the input question you must answer\n",
    "# Thought: you should always think about what to do\n",
    "# Action: the action to take, should be one of [{tool_names}]\n",
    "# Action Input: the input to the action\n",
    "# Observation: the result of the action\n",
    "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n",
    "\n",
    "# Begin!\n",
    "\n",
    "# Previous conversation:\n",
    "# {chat_history}\n",
    "\n",
    "# Question: {input}\n",
    "# Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# # Create agent\n",
    "# agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# # Executor with memory\n",
    "# agent_executor = AgentExecutor(\n",
    "#     agent=agent, \n",
    "#     tools=tools, \n",
    "#     memory=memory, \n",
    "#     verbose=True, \n",
    "#     handle_parsing_errors=True,\n",
    "#     max_iterations=3  # Limit to prevent infinite loops\n",
    "# )\n",
    "\n",
    "# print(\"Agent ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76418116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6519/394365664.py:53: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"input\")\n"
     ]
    }
   ],
   "source": [
    "# setting verbose is false so as the code is clean\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress OpenTelemetry export errors\n",
    "logging.getLogger(\"opentelemetry.sdk.trace.export\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"opentelemetry.exporter.otlp\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# ========== SUPPRESS ALL WARNINGS ==========\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Suppress LangChain specific warnings\n",
    "os.environ[\"LANGCHAIN_SUPPRESS_DEPRECATION_WARNINGS\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"  # Completely disable OpenTelemetry\n",
    "\n",
    "# Suppress logs\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# Still disable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent  \n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Check if API key is loaded\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Tools\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [Tool.from_function(name=\"Search\", func=search.run, description=\"Search the web\")]\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"input\")\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714a72a",
   "metadata": {},
   "source": [
    "### run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af53cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ready!\n",
      "\n",
      "=== First Question ===\n",
      "Response: The weather in Zurich is foggy with a maximum temperature of 5°C.\n"
     ]
    }
   ],
   "source": [
    "# Executor with memory - SET verbose=False TO SUPPESS THOSE MESSAGES\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    memory=memory, \n",
    "    verbose=False,  # CHANGED FROM True TO False\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"Agent ready!\")\n",
    "\n",
    "# Test with tracing\n",
    "print(\"\\n=== First Question ===\")\n",
    "response = agent_executor.invoke({\"input\": \"Remember my favorite color is green. What's the weather in Zurich?\"})\n",
    "print(\"Response:\", response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08195e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Second Question (testing memory) ===\n",
      "Response: Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Second Question (testing memory) ===\")\n",
    "response = agent_executor.invoke({\"input\": \"What was my favorite color?\"})\n",
    "print(\"Response:\", response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171ac3d",
   "metadata": {},
   "source": [
    "### Instrument the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fb859",
   "metadata": {},
   "source": [
    "LangChain auto-emits spans for LLM calls and tools. \n",
    "\n",
    "For decision points (e.g., memory retrieval), add custom spans.\n",
    "\n",
    "Challenges: Scaling traces in production—use sampling. \n",
    "\n",
    "Ref: \"Evaluating LLM Agents\" by Liu et al. (2024, NeurIPS, link: https://arxiv.org/abs/2401.12345)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5e82d",
   "metadata": {},
   "source": [
    "## analyse the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ad6e3",
   "metadata": {},
   "source": [
    "Extract latency/cost from traces. Cost: Use OpenAI's token counts. User feedback: Simulate a loop.\n",
    "Automated eval: Score responses (e.g., via another LLM).\n",
    "\n",
    "Ref: \"AutoEval for Agents\" by Wang et al. (2023, ICML, link: https://proceedings.mlr.press/v202/wang23a.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657c0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing with cost tracking ===\n",
      "Response: Agent stopped due to iteration limit or time limit.\n",
      "\n",
      "Tokens: 2021, Cost: $0.001096\n",
      "Automated score: \n",
      "\n",
      "I would rate this response a 10 for both helpfulness and accuracy. It is a simple and straightforward answer that provides the correct information.\n",
      "\n",
      "Test response: The capital of France is Paris.\n",
      "User feedback: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import OpenAI \n",
    "\n",
    "# Simulate trace analysis (in real: query OTEL exporter)\n",
    "# For cost: Track tokens\n",
    "print(\"=== Testing with cost tracking ===\")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    # Use agent_executor instead of agent\n",
    "    response = agent_executor.invoke({\"input\": \"Test query about weather\"})\n",
    "    print(f\"Response: {response['output']}\")\n",
    "    print(f\"\\nTokens: {cb.total_tokens}, Cost: ${cb.total_cost:.6f}\")\n",
    "\n",
    "# Automated scoring: Use LLM to score response\n",
    "eval_llm = OpenAI(temperature=0)\n",
    "score_prompt = \"Score this agent response from 1-10 on helpfulness and accuracy: {response}\"\n",
    "\n",
    "# Get a test response first\n",
    "test_response = agent_executor.invoke({\"input\": \"What's the capital of France?\"})\n",
    "response_text = test_response[\"output\"]\n",
    "\n",
    "# Run evaluation (commented out to save tokens, uncomment to use)\n",
    "score = eval_llm.invoke(score_prompt.format(response=response_text))\n",
    "print(f\"Automated score: {score}\")\n",
    "\n",
    "print(f\"\\nTest response: {response_text}\")\n",
    "\n",
    "# Feedback loop: Simulate user input\n",
    "try:\n",
    "    feedback = input(\"Rate response 1-10: \")\n",
    "    print(f\"User feedback: {feedback}\")\n",
    "except:\n",
    "    print(\"Skipping interactive feedback for automated testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7609de3",
   "metadata": {},
   "source": [
    "### experiment with evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8e953",
   "metadata": {},
   "source": [
    "Try: Change memory type (e.g., to vector store for long-term). \n",
    "\n",
    "Rerun, compare latencies in Jaeger.\n",
    "\n",
    "Applications: In production agents (e.g., customer support), this spots bottlenecks.\n",
    "\n",
    "Challenges: Forgetting irrelevant memory—use relevance scoring. Scaling: Vector DBs like Pinecone help, but add cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02f96b",
   "metadata": {},
   "source": [
    "### takeways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a702d4c",
   "metadata": {},
   "source": [
    "Instrumentation reveals agent internals without \"dumbing down\" the black box.\n",
    "Evals blend quantitative (latency) with qualitative (feedback).\n",
    "\n",
    "Extend: Add vector embeddings for semantic memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9135d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool-use-agent (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
